# Proyecto Bigdata-03 - Sistema Chinook Data Lake

Proyecto completo de an√°lisis de datos con sistema transaccional, data lake y ETLs para extracci√≥n de insights de ventas de m√∫sica.

## üìã Descripci√≥n General

Este proyecto implementa una soluci√≥n completa de Big Data que incluye:

1. **Sistema Transaccional**: Aplicaci√≥n web FastAPI + React para ventas de m√∫sica
2. **Data Lake en S3**: Almacenamiento anal√≠tico con arquitectura RAW ‚Üí PROCESSED ‚Üí ANALYTICS
3. **ETLs**: Procesos de extracci√≥n, transformaci√≥n y carga de datos
4. **AWS Glue**: Catalogaci√≥n autom√°tica de datos
5. **Amazon Athena**: Consultas SQL sobre el data lake

## üèóÔ∏è Arquitectura

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   WebApp        ‚îÇ      ‚îÇ   RDS/MySQL  ‚îÇ      ‚îÇ   S3 Bucket     ‚îÇ
‚îÇ  (FastAPI +     ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Chinook DB  ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ  Data Lake      ‚îÇ
‚îÇ   React)        ‚îÇ      ‚îÇ              ‚îÇ ETLs ‚îÇ                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                ‚îÇ                        ‚îÇ
                                ‚îÇ                        ‚îÇ
                                ‚ñº                        ‚ñº
                         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                         ‚îÇ  AWS Glue    ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Amazon Athena  ‚îÇ
                         ‚îÇ  Crawler     ‚îÇ      ‚îÇ  (Consultas)    ‚îÇ
                         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## üìÅ Estructura del Proyecto

```
Bigdata-03/
‚îú‚îÄ‚îÄ .github/                   # GitHub Actions workflows
‚îÇ   ‚îî‚îÄ‚îÄ workflows/
‚îÇ       ‚îú‚îÄ‚îÄ ci-cd-pipeline.yml       # Pipeline principal CI/CD
‚îÇ       ‚îú‚îÄ‚îÄ blank.yml                # Template b√°sico
‚îÇ       ‚îî‚îÄ‚îÄ blankexample.yml         # Ejemplo de referencia
‚îÇ
‚îú‚îÄ‚îÄ WebApp/                    # Aplicaci√≥n web transaccional
‚îÇ   ‚îú‚îÄ‚îÄ frontend/             # React + TypeScript
‚îÇ   ‚îú‚îÄ‚îÄ backend/              # FastAPI + SQLAlchemy
‚îÇ   ‚îî‚îÄ‚îÄ docker-compose.yml    # Orquestaci√≥n de contenedores
‚îÇ
‚îú‚îÄ‚îÄ etls/                      # ETLs para el data lake
‚îÇ   ‚îú‚îÄ‚îÄ etl_0_full_copy.py    # Copia completa RDS ‚Üí S3
‚îÇ   ‚îú‚îÄ‚îÄ etl_1_ventas_por_dia.py        # An√°lisis diario
‚îÇ   ‚îú‚îÄ‚îÄ etl_2_artista_mas_vendido.py   # Top artistas/mes
‚îÇ   ‚îú‚îÄ‚îÄ etl_3_dia_semana.py            # An√°lisis por d√≠a semana
‚îÇ   ‚îú‚îÄ‚îÄ etl_4_mes_mayor_ventas.py      # An√°lisis mensual
‚îÇ   ‚îî‚îÄ‚îÄ README.md             # Documentaci√≥n detallada ETLs
‚îÇ
‚îú‚îÄ‚îÄ tests/                     # Tests unitarios
‚îÇ   ‚îú‚îÄ‚îÄ test_data_lake.py     # Tests para data_lake.py
‚îÇ   ‚îú‚îÄ‚îÄ test_crawler.py       # Tests para crawler.py
‚îÇ   ‚îú‚îÄ‚îÄ test_etls.py          # Tests para ETLs
‚îÇ   ‚îú‚îÄ‚îÄ test_emr_clustering.py       # Tests para clustering
‚îÇ   ‚îî‚îÄ‚îÄ test_setup_emr_s3.py         # Tests para S3 setup
‚îÇ
‚îú‚îÄ‚îÄ data_lake.py              # Creaci√≥n de estructura S3
‚îú‚îÄ‚îÄ crawler.py                # Configuraci√≥n AWS Glue
‚îú‚îÄ‚îÄ emr_clustering.py         # Pipeline de ML con KMeans
‚îú‚îÄ‚îÄ setup_emr_s3.py           # Setup de S3 para EMR
‚îú‚îÄ‚îÄ requirements.txt          # Dependencias Python
‚îú‚îÄ‚îÄ verify_setup.sh           # Script de verificaci√≥n
‚îú‚îÄ‚îÄ GITHUB_ACTIONS_SETUP.md   # Gu√≠a de CI/CD
‚îú‚îÄ‚îÄ EMR_CLUSTERING.md         # Documentaci√≥n clustering
‚îú‚îÄ‚îÄ instrucciones.md          # Especificaciones del proyecto
‚îî‚îÄ‚îÄ README.md                 # Este archivo
```

## ‚úÖ Punto 1: Aplicaci√≥n Web (WebApp/)

### Frontend
- **Framework**: React + TypeScript + Vite
- **Caracter√≠sticas**:
  - Cat√°logo de canciones con b√∫squeda y filtros
  - Carrito de compras
  - Navegaci√≥n por artistas y √°lbumes
  - Dise√±o responsive

### Backend
- **Framework**: FastAPI + SQLAlchemy
- **Base de Datos**: MySQL (Chinook)
- **Endpoints**:
  - `/tracks`: B√∫squeda de canciones
  - `/artists`: Listado de artistas
  - `/albums`: √Ålbumes por artista
  - `/cart`: Gesti√≥n de carrito

### Despliegue
```bash
cd WebApp
docker-compose up
```

## ‚úÖ Punto 2: Data Lake en S3

### Estructura
```
chinook-datalake/
‚îú‚îÄ‚îÄ raw/                      # Datos crudos desde RDS
‚îÇ   ‚îú‚îÄ‚îÄ Artist/
‚îÇ   ‚îú‚îÄ‚îÄ Album/
‚îÇ   ‚îú‚îÄ‚îÄ Track/
‚îÇ   ‚îú‚îÄ‚îÄ Invoice/
‚îÇ   ‚îú‚îÄ‚îÄ InvoiceLine/
‚îÇ   ‚îî‚îÄ‚îÄ customer_employee_history/
‚îÇ
‚îú‚îÄ‚îÄ processed/                # Datos procesados por ETLs
‚îÇ   ‚îú‚îÄ‚îÄ ventas_por_dia/      # M√©tricas diarias
‚îÇ   ‚îú‚îÄ‚îÄ artista_mes/          # Top artistas mensuales
‚îÇ   ‚îú‚îÄ‚îÄ dia_semana/           # An√°lisis d√≠a de semana
‚îÇ   ‚îî‚îÄ‚îÄ mes_ventas/           # An√°lisis mensual
‚îÇ
‚îî‚îÄ‚îÄ analytics/                # Reportes y dashboards
    ‚îî‚îÄ‚îÄ informes/

chinook-emr-scripts/         # Scripts para EMR
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îî‚îÄ‚îÄ clustering/
‚îÇ       ‚îî‚îÄ‚îÄ customer_clustering.py
‚îú‚îÄ‚îÄ logs/
‚îú‚îÄ‚îÄ output/
‚îî‚îÄ‚îÄ notebooks/
```

### M√≥dulos

**`data_lake.py`**: Crea la estructura completa del data lake en S3
```python
from data_lake import create_data_lake

result = create_data_lake('chinook-datalake')
```

**`crawler.py`**: Configura AWS Glue para catalogar datos
```python
from crawler import setup_glue_crawler

result = setup_glue_crawler('chinook-datalake', 'chinook_db')
```

### Athena
Los datos en formato Parquet son autom√°ticamente consultables:
```sql
SELECT * FROM etl_ventas_por_dia 
WHERE fecha > '2024-01-01'
ORDER BY total_canciones_vendidas DESC;
```

## ‚úÖ Punto 3: ETLs con AWS Glue

### ETL 0 - Full Copy
**Prop√≥sito**: Snapshot completo de todas las tablas

```python
from etls import run_full_copy_etl

db_config = {
    'host': 'localhost',
    'user': 'root',
    'password': 'password',
    'database': 'chinook'
}

result = run_full_copy_etl(db_config, 'chinook-datalake')
```

**Salida**: 
- Todas las tablas en `raw/{tabla}/`
- Hist√≥rico cliente-empleado con jerarqu√≠a

### ETL 1 - Ventas por D√≠a
**M√©tricas**:
- Total canciones vendidas por d√≠a
- N√∫mero de facturas y clientes √∫nicos
- Monto total y ticket promedio
- D√≠a de semana, mes, a√±o, trimestre

### ETL 2 - Artista M√°s Vendido por Mes
**M√©tricas**:
- Top artista de cada mes
- Participaci√≥n porcentual
- Ranking top 5 artistas por mes

### ETL 3 - D√≠a de la Semana con M√°s Ventas
**M√©tricas**:
- Ventas por d√≠a de la semana
- Comparaci√≥n semana vs fin de semana
- Promedios normalizados

### ETL 4 - Mes con Mayor Volumen
**M√©tricas**:
- Ranking hist√≥rico de meses
- Mejores meses por a√±o
- Mejor trimestre
- An√°lisis de tendencias

## üß™ Tests

### Ejecutar Tests Localmente
```bash
# Verificar configuraci√≥n completa
./verify_setup.sh

# Todos los tests
PYTHONPATH=$(pwd) pytest tests/ -v

# Tests espec√≠ficos
PYTHONPATH=$(pwd) pytest tests/test_etls.py -v
PYTHONPATH=$(pwd) pytest tests/test_data_lake.py -v
PYTHONPATH=$(pwd) pytest tests/test_emr_clustering.py -v
```

### Cobertura
- ‚úÖ **54 tests** en total (24 ETL/infra + 30 EMR/S3)
- ‚úÖ 100% de √©xito
- ‚úÖ Cobertura de ETLs, data lake, crawler, clustering y S3
- ‚úÖ Uso de mocking para AWS (moto)
- ‚úÖ Tests de PySpark con local[2]

```
================================ 54 passed ================================
```

## üöÄ CI/CD con GitHub Actions

### Configuraci√≥n Autom√°tica

El proyecto incluye un pipeline completo de CI/CD que se ejecuta autom√°ticamente en cada push a `main`:

```yaml
Test ‚Üí Deploy Data Lake ‚Üí Deploy EMR Scripts ‚Üí Deploy ETLs ‚Üí Notify
```

### Workflows Disponibles

1. **ci-cd-pipeline.yml** (Principal)
   - ‚úÖ Ejecuta todos los tests con pytest
   - üìä Genera reportes de cobertura
   - üóÑÔ∏è Despliega estructura de Data Lake en S3
   - üï∑Ô∏è Crea Glue Crawlers
   - üöÄ Sube scripts EMR a S3
   - üìä Despliega ETLs a S3

2. **blankexample.yml** (Referencia)
   - Ejemplo de deployment de Glue jobs

### Configurar CI/CD

```bash
# 1. Ver gu√≠a completa
cat GITHUB_ACTIONS_SETUP.md

# 2. Configurar secretos en GitHub:
# - AWS_ACCESS_KEY_ID
# - AWS_SECRET_ACCESS_KEY
# - AWS_SESSION_TOKEN
# - DATA_LAKE_BUCKET_NAME
# - EMR_SCRIPTS_BUCKET_NAME
# - GLUE_DATABASE_NAME
# - GLUE_ROLE_ARN

# 3. Push para activar pipeline
git add .
git commit -m "chore: Activar CI/CD"
git push origin main
```

Ver documentaci√≥n completa en [GITHUB_ACTIONS_SETUP.md](GITHUB_ACTIONS_SETUP.md)

## üöÄ Instalaci√≥n y Configuraci√≥n

### Prerrequisitos
- Python 3.12+
- Node.js 18+
- Docker y Docker Compose
- AWS CLI configurado
- Cuenta AWS con permisos para S3, Glue, Athena

### Instalaci√≥n

1. **Clonar repositorio**
```bash
git clone https://github.com/coffool/Bigdata-03.git
cd Bigdata-03
```

2. **Configurar entorno Python**
```bash
python -m venv .venv
source .venv/bin/activate  # Linux/Mac
pip install -r requirements.txt
```

3. **Instalar dependencias**
```bash
pip install boto3 pymysql pandas pyarrow pytest moto
```

4. **Configurar variables de entorno**
```bash
export DB_HOST="localhost"
export DB_USER="root"
export DB_PASSWORD="password"
export DB_NAME="chinook"
export DB_PORT="3306"
export S3_BUCKET="chinook-datalake"
```

5. **Iniciar aplicaci√≥n web**
```bash
cd WebApp
docker-compose up -d
```

## üìä Uso

### Crear Data Lake
```bash
python data_lake.py
```

### Configurar Crawler
```bash
python crawler.py
```

### Ejecutar ETLs
```bash
# ETL individual
python etls/etl_1_ventas_por_dia.py

# Todos los ETLs (crear script)
for etl in etls/etl_*.py; do
    python $etl
done
```

### Consultar en Athena
```sql
-- Ver ventas por d√≠a
SELECT fecha, total_canciones_vendidas, monto_total
FROM etl_ventas_por_dia
ORDER BY fecha DESC
LIMIT 10;

-- Top artistas por mes
SELECT mes, nombre_artista, total_canciones_vendidas
FROM etl_top_artista_por_mes
ORDER BY mes DESC, total_canciones_vendidas DESC;
```

## üìà M√©tricas del Proyecto

- **4 Puntos Completos**: WebApp + Data Lake + ETLs + EMR Clustering
- **5 ETLs** completamente funcionales
- **54 tests** con 100% de √©xito (24 infra + 30 EMR)
- **Pipeline CI/CD** automatizado con GitHub Actions
- **Arquitectura escalable** y modular
- **Documentaci√≥n completa** con ejemplos
- **Formato Parquet** para optimizaci√≥n
- **Compatible con Athena** out-of-the-box
- **ML Pipeline** con PySpark KMeans clustering

## ‚úÖ Punto 4: EMR Clustering (Nuevo)

### Customer Clustering con PySpark

Pipeline completo de Machine Learning para agrupar clientes seg√∫n sus preferencias musicales:

**Features Analizadas** (12 total):
- Total de compras y gasto
- Precio y duraci√≥n promedio de canciones
- Diversidad de g√©neros (unique_genres)
- Porcentajes por g√©nero: Rock, Metal, Jazz, Latin, Blues, Classical, Otros

**Algoritmo**: KMeans con StandardScaler
**Evaluaci√≥n**: Silhouette Score + m√©todo del codo

### Uso

```bash
# Setup de S3 para EMR
python setup_emr_s3.py \
  --bucket-name chinook-emr-scripts \
  --script-path emr_clustering.py

# Ejecutar clustering en EMR
spark-submit \
  --master yarn \
  s3://chinook-emr-scripts/scripts/clustering/customer_clustering.py \
  --data-source s3 \
  --s3-bucket chinook-datalake \
  --k 5
```

Ver documentaci√≥n completa en [EMR_CLUSTERING.md](EMR_CLUSTERING.md)

## üîß Tecnolog√≠as Utilizadas

### Backend
- Python 3.12
- FastAPI
- SQLAlchemy
- Pandas
- Boto3 (AWS SDK)
- PyMySQL

### Frontend
- React 18
- TypeScript
- Vite
- Axios

### Infraestructura
- Docker & Docker Compose
- AWS S3
- AWS Glue
- Amazon Athena
- MySQL/RDS

### Testing
- pytest
- moto (AWS mocking)
- unittest.mock

## üìù Documentaci√≥n Adicional

- **[GITHUB_ACTIONS_SETUP.md](GITHUB_ACTIONS_SETUP.md)** - Gu√≠a completa de CI/CD
- **[EMR_CLUSTERING.md](EMR_CLUSTERING.md)** - Documentaci√≥n de clustering
- **[etls/README.md](etls/README.md)** - Documentaci√≥n detallada de ETLs
- **[instrucciones.md](instrucciones.md)** - Especificaciones originales
- **[WebApp/README.md](WebApp/README.md)** - Documentaci√≥n de la aplicaci√≥n web
- **[DEPLOYMENT.md](WebApp/DEPLOYMENT.md)** - Gu√≠a de despliegue en EC2

## ü§ù Contribuci√≥n

Este es un proyecto acad√©mico. Para sugerencias o mejoras, crear un issue o pull request.

## üìÑ Licencia

Este proyecto es parte de un trabajo acad√©mico de Big Data.

## üë• Autor

Proyecto desarrollado como parte del curso de Big Data - 2025