name: CI/CD Pipeline - Bigdata-03

on:
  push:
    branches: ["main"]
  pull_request:
    branches: ["main"]
  workflow_dispatch:

jobs:
  test:
    name: ğŸ§ª Test & Build
    runs-on: ubuntu-latest
    environment: development

    steps:
      - name: ğŸ§° Checkout repository
        uses: actions/checkout@v4

      - name: ğŸ Set up Python 3.12
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: âš¡ Cache pip dependencies
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: ğŸ“¦ Install dependencies
        run: |
          python -m pip install --upgrade pip setuptools wheel
          pip install -r requirements.txt

      - name: ğŸ§ª Run tests
        env:
          PYTHONPATH: ${{ github.workspace }}
        run: |
          echo "Running all tests..."
          pytest tests/ -v --maxfail=5 --disable-warnings --tb=short
          echo "âœ… All tests passed!"

      - name: ğŸ“Š Generate coverage report
        if: always()
        env:
          PYTHONPATH: ${{ github.workspace }}
        run: |
          pytest tests/ --cov=. --cov-report=xml --cov-report=html --cov-report=term
          echo "âœ… Coverage report generated!"

      - name: ğŸ“¤ Upload coverage reports
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: coverage-reports
          path: |
            htmlcov/
            coverage.xml

      - name: ğŸ“¦ Package artifacts
        run: |
          echo "Packaging ETL scripts..."
          mkdir -p artifacts
          cp -r etls/ artifacts/
          cp -r data_lake.py crawler.py artifacts/
          cp emr_clustering.py setup_emr_s3.py artifacts/
          cp requirements.txt artifacts/
          tar -czf artifacts.tar.gz artifacts/
          echo "âœ… Artifacts packaged!"

      - name: ğŸ“¤ Upload artifacts
        uses: actions/upload-artifact@v4
        with:
          name: deployment-package
          path: artifacts.tar.gz
          retention-days: 30

  deploy-data-lake:
    name: ğŸ—„ï¸ Deploy Data Lake
    runs-on: ubuntu-latest
    needs: test
    if: github.ref == 'refs/heads/main' && github.event_name == 'push'
    environment: production

    steps:
      - name: ğŸ§° Checkout repository
        uses: actions/checkout@v4

      - name: ğŸ Set up Python 3.12
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: ğŸ“¦ Install boto3
        run: |
          pip install boto3 botocore

      - name: ğŸ” Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-session-token: ${{ secrets.AWS_SESSION_TOKEN }}
          aws-region: us-east-1

      - name: ğŸ—„ï¸ Create S3 Data Lake structure
        env:
          AWS_DEFAULT_REGION: us-east-1
          DATA_LAKE_BUCKET: ${{ secrets.DATA_LAKE_BUCKET_NAME }}
        run: |
          echo "ğŸ—„ï¸ Creating Data Lake structure in S3..."
          python -c "from data_lake import create_data_lake; result = create_data_lake('${{ secrets.DATA_LAKE_BUCKET_NAME }}'); print(f'Created {len(result[\"folders_created\"])} folders')"
          echo "âœ… Data Lake structure created!"

      - name: ğŸ•·ï¸ Create Glue Crawlers
        env:
          AWS_DEFAULT_REGION: us-east-1
          DATA_LAKE_BUCKET: ${{ secrets.DATA_LAKE_BUCKET_NAME }}
          GLUE_DATABASE: ${{ secrets.GLUE_DATABASE_NAME }}
          GLUE_ROLE_ARN: ${{ secrets.GLUE_ROLE_ARN }}
        run: |
          echo "ğŸ•·ï¸ Creating Glue Crawlers..."
          python -c "from crawler import setup_glue_crawler; result = setup_glue_crawler('${{ secrets.DATA_LAKE_BUCKET_NAME }}', '${{ secrets.GLUE_DATABASE_NAME }}', 'chinook-crawler', '${{ secrets.GLUE_ROLE_ARN }}'); print(f'Database: {result[\"database_created\"]}, Crawler: {result[\"crawler_created\"]}')"  
          echo "âœ… Crawlers created!"

  deploy-emr-scripts:
    name: ğŸš€ Deploy EMR Scripts to S3
    runs-on: ubuntu-latest
    needs: test
    if: github.ref == 'refs/heads/main' && github.event_name == 'push'
    environment: production

    steps:
      - name: ğŸ§° Checkout repository
        uses: actions/checkout@v4

      - name: ğŸ“¥ Download deployment package
        uses: actions/download-artifact@v4
        with:
          name: deployment-package
          path: .

      - name: ğŸ Set up Python 3.12
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: ğŸ“¦ Install dependencies
        run: |
          pip install boto3 botocore

      - name: ğŸ” Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-session-token: ${{ secrets.AWS_SESSION_TOKEN }}
          aws-region: us-east-1

      - name: ğŸ“¤ Upload EMR clustering script to S3
        env:
          AWS_DEFAULT_REGION: us-east-1
          EMR_SCRIPTS_BUCKET: ${{ secrets.EMR_SCRIPTS_BUCKET_NAME }}
        run: |
          echo "ğŸ“¤ Uploading EMR clustering script to S3..."
          python setup_emr_s3.py \
            --bucket-name ${EMR_SCRIPTS_BUCKET} \
            --script-path emr_clustering.py \
            --region us-east-1
          echo "âœ… EMR scripts uploaded!"

  deploy-etls:
    name: ğŸ“Š Deploy ETL Scripts
    runs-on: ubuntu-latest
    needs: [test, deploy-data-lake]
    if: github.ref == 'refs/heads/main' && github.event_name == 'push'
    environment: production

    steps:
      - name: ğŸ§° Checkout repository
        uses: actions/checkout@v4

      - name: ğŸ“¥ Download deployment package
        uses: actions/download-artifact@v4
        with:
          name: deployment-package
          path: .

      - name: ğŸ Set up Python 3.12
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: ğŸ“¦ Install dependencies
        run: |
          pip install -r requirements.txt
          pip install boto3 botocore

      - name: ğŸ” Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-session-token: ${{ secrets.AWS_SESSION_TOKEN }}
          aws-region: us-east-1

      - name: ğŸ“¤ Upload ETL scripts to S3
        env:
          AWS_DEFAULT_REGION: us-east-1
          DATA_LAKE_BUCKET: ${{ secrets.DATA_LAKE_BUCKET_NAME }}
        run: |
          echo "ğŸ“¤ Uploading ETL scripts to S3..."
          tar -xzf artifacts.tar.gz
          
          # Upload ETL scripts and constants
          aws s3 sync artifacts/etls/ s3://${DATA_LAKE_BUCKET}/scripts/etls/ \
            --exclude "*.pyc" \
            --exclude "__pycache__/*"
          
          # Upload support scripts
          aws s3 cp artifacts/data_lake.py s3://${DATA_LAKE_BUCKET}/scripts/
          aws s3 cp artifacts/crawler.py s3://${DATA_LAKE_BUCKET}/scripts/
          
          echo "âœ… ETL scripts uploaded!"

      - name: ğŸ“‹ List deployed resources
        run: |
          echo "ğŸ“‹ Listing deployed resources..."
          echo ""
          echo "=== S3 Buckets ==="
          aws s3 ls
          echo ""
          echo "=== Data Lake Structure ==="
          aws s3 ls s3://${DATA_LAKE_BUCKET}/ --recursive | head -20
          echo ""
          echo "=== Glue Crawlers ==="
          aws glue list-crawlers --query "CrawlerNames" --output table
          echo ""
          echo "âœ… Deployment summary complete!"
        env:
          DATA_LAKE_BUCKET: ${{ secrets.DATA_LAKE_BUCKET_NAME }}

  notify:
    name: ğŸ“¢ Notify Deployment Status
    runs-on: ubuntu-latest
    needs: [deploy-data-lake, deploy-emr-scripts, deploy-etls]
    if: always() && github.ref == 'refs/heads/main'
    
    steps:
      - name: ğŸ“Š Check deployment status
        run: |
          echo "ğŸ‰ Deployment completed!"
          echo "Status: ${{ needs.deploy-data-lake.result }}"
          echo "Data Lake: ${{ needs.deploy-data-lake.result }}"
          echo "EMR Scripts: ${{ needs.deploy-emr-scripts.result }}"
          echo "ETL Scripts: ${{ needs.deploy-etls.result }}"
